{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import shelve\n",
    "import random\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "import model_1\n",
    "_ = importlib.reload(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "BATCH_SIZE = 16\n",
    "SHELVE_PATH = 'data/processed-data/data-1/db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = model_1.ShelveDataset(SHELVE_PATH)\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=model_1.ShelveDataset.collate_fn, \n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_1.AvgPoolingCnn().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100  ][Batch 0  ]\tLoss: 7.463075160980225\n",
      "[Epoch 1/100  ][Batch 1  ]\tLoss: 54.79018020629883\n",
      "[Epoch 1/100  ][Batch 2  ]\tLoss: 66.20771789550781\n",
      "[Epoch 1/100  ][Batch 3  ]\tLoss: 84.72021484375\n",
      "[Epoch 1/100  ][Batch 4  ]\tLoss: 100.67752075195312\n",
      "[Epoch 1/100  ][Batch 5  ]\tLoss: 129.2393035888672\n",
      "[Epoch 1/100  ][Batch 6  ]\tLoss: 112.7170639038086\n",
      "[Epoch 1/100  ][Batch 7  ]\tLoss: 114.82838439941406\n",
      "[Epoch 1/100  ][Batch 8  ]\tLoss: 97.14972686767578\n",
      "[Epoch 1/100  ][Batch 9  ]\tLoss: 72.88104248046875\n",
      "[Epoch 1/100  ][Batch 10 ]\tLoss: 212.95040893554688\n",
      "[Epoch 1/100  ][Batch 11 ]\tLoss: 46.761695861816406\n",
      "[Epoch 1/100  ][Batch 12 ]\tLoss: 60.223731994628906\n",
      "[Epoch 1/100  ][Batch 13 ]\tLoss: 51.31937789916992\n",
      "[Epoch 1/100  ][Batch 14 ]\tLoss: 82.19158172607422\n",
      "[Epoch 1/100  ][Batch 15 ]\tLoss: 51.464229583740234\n",
      "[Epoch 1/100  ][Batch 16 ]\tLoss: 52.60686492919922\n",
      "[Epoch 1/100  ][Batch 17 ]\tLoss: 76.40847778320312\n",
      "[Epoch 1/100  ][Batch 18 ]\tLoss: 294.6927185058594\n",
      "[Epoch 1/100  ][Batch 19 ]\tLoss: 28.203128814697266\n",
      "[Epoch 1/100  ][Batch 20 ]\tLoss: 37.258628845214844\n",
      "[Epoch 1/100  ][Batch 21 ]\tLoss: 27.458030700683594\n",
      "[Epoch 1/100  ][Batch 22 ]\tLoss: 59.483192443847656\n",
      "[Epoch 1/100  ][Batch 23 ]\tLoss: 45.0532112121582\n",
      "[Epoch 1/100  ][Batch 24 ]\tLoss: 60.82453155517578\n",
      "[Epoch 1/100  ][Batch 25 ]\tLoss: 176.8291473388672\n",
      "[Epoch 1/100  ][Batch 26 ]\tLoss: 37.9587516784668\n",
      "[Epoch 1/100  ][Batch 27 ]\tLoss: 35.92106246948242\n",
      "[Epoch 1/100  ][Batch 28 ]\tLoss: 24.581039428710938\n",
      "[Epoch 1/100  ][Batch 29 ]\tLoss: 28.532150268554688\n",
      "[Epoch 1/100  ][Batch 30 ]\tLoss: 26.97896385192871\n",
      "[Epoch 1/100  ][Batch 31 ]\tLoss: 42.56913375854492\n",
      "[Epoch 1/100  ][Batch 32 ]\tLoss: 37.34535598754883\n",
      "[Epoch 1/100  ][Batch 33 ]\tLoss: 68.06636047363281\n",
      "[Epoch 1/100  ][Batch 34 ]\tLoss: 40.504356384277344\n",
      "[Epoch 1/100  ][Batch 35 ]\tLoss: 17.498641967773438\n",
      "[Epoch 1/100  ][Batch 36 ]\tLoss: 16.825214385986328\n",
      "[Epoch 1/100  ][Batch 37 ]\tLoss: 10.646747589111328\n",
      "[Epoch 1/100  ][Batch 38 ]\tLoss: 15.312451362609863\n",
      "[Epoch 1/100  ][Batch 39 ]\tLoss: 40.739402770996094\n",
      "[Epoch 1/100  ][Batch 40 ]\tLoss: 11.821450233459473\n",
      "[Epoch 1/100  ][Batch 41 ]\tLoss: 33.59761047363281\n",
      "[Epoch 1/100  ][Batch 42 ]\tLoss: 20.195554733276367\n",
      "[Epoch 1/100  ][Batch 43 ]\tLoss: 12.822687149047852\n",
      "[Epoch 1/100  ][Batch 44 ]\tLoss: 12.685471534729004\n",
      "[Epoch 1/100  ][Batch 45 ]\tLoss: 13.82333755493164\n",
      "[Epoch 1/100  ][Batch 46 ]\tLoss: 9.602794647216797\n",
      "[Epoch 1/100  ][Batch 47 ]\tLoss: 38.79227066040039\n",
      "[Epoch 1/100  ][Batch 48 ]\tLoss: 8.227374076843262\n",
      "[Epoch 1/100  ][Batch 49 ]\tLoss: 43.66215133666992\n",
      "[Epoch 1/100  ][Batch 50 ]\tLoss: 6.170948505401611\n",
      "[Epoch 1/100  ][Batch 51 ]\tLoss: 29.696697235107422\n",
      "[Epoch 1/100  ][Batch 52 ]\tLoss: 12.360437393188477\n",
      "[Epoch 1/100  ][Batch 53 ]\tLoss: 9.491007804870605\n",
      "[Epoch 1/100  ][Batch 54 ]\tLoss: 16.012283325195312\n",
      "[Epoch 1/100  ][Batch 55 ]\tLoss: 14.836838722229004\n",
      "[Epoch 1/100  ][Batch 56 ]\tLoss: 9.221953392028809\n",
      "[Epoch 1/100  ][Batch 57 ]\tLoss: 22.4560546875\n",
      "[Epoch 1/100  ][Batch 58 ]\tLoss: 7.338987827301025\n",
      "[Epoch 1/100  ][Batch 59 ]\tLoss: 11.05538272857666\n",
      "[Epoch 1/100  ][Batch 60 ]\tLoss: 12.748924255371094\n",
      "[Epoch 1/100  ][Batch 61 ]\tLoss: 4.185399532318115\n",
      "[Epoch 1/100  ][Batch 62 ]\tLoss: 11.564199447631836\n",
      "[Epoch 1/100  ][Batch 63 ]\tLoss: 2.00593900680542\n",
      "[Epoch 1/100  ][Batch 64 ]\tLoss: 11.85480785369873\n",
      "[Epoch 1/100  ][Batch 65 ]\tLoss: 5.627195835113525\n",
      "[Epoch 1/100  ][Batch 66 ]\tLoss: 3.412036895751953\n",
      "[Epoch 1/100  ][Batch 67 ]\tLoss: 4.752546310424805\n",
      "[Epoch 1/100  ][Batch 68 ]\tLoss: 74.78136444091797\n",
      "[Epoch 1/100  ][Batch 69 ]\tLoss: 5.95880126953125\n",
      "[Epoch 1/100  ][Batch 70 ]\tLoss: 8.588865280151367\n",
      "[Epoch 1/100  ][Batch 71 ]\tLoss: 12.16715145111084\n",
      "[Epoch 1/100  ][Batch 72 ]\tLoss: 21.334056854248047\n",
      "[Epoch 1/100  ][Batch 73 ]\tLoss: 11.11124324798584\n",
      "[Epoch 1/100  ][Batch 74 ]\tLoss: 5.389669895172119\n",
      "[Epoch 1/100  ][Batch 75 ]\tLoss: 7.811498641967773\n",
      "[Epoch 1/100  ][Batch 76 ]\tLoss: 3.487665891647339\n",
      "[Epoch 1/100  ][Batch 77 ]\tLoss: 3.110999345779419\n",
      "[Epoch 1/100  ][Batch 78 ]\tLoss: 6.477281093597412\n",
      "[Epoch 1/100  ][Batch 79 ]\tLoss: 3.7147061824798584\n",
      "[Epoch 1/100  ][Batch 80 ]\tLoss: 4.788969993591309\n",
      "[Epoch 1/100  ][Batch 81 ]\tLoss: 16.66683006286621\n",
      "[Epoch 1/100  ][Batch 82 ]\tLoss: 0.8777585625648499\n",
      "[Epoch 1/100  ][Batch 83 ]\tLoss: 17.741004943847656\n",
      "[Epoch 1/100  ][Batch 84 ]\tLoss: 5.176668643951416\n",
      "[Epoch 1/100  ][Batch 85 ]\tLoss: 3.733945846557617\n",
      "[Epoch 1/100  ][Batch 86 ]\tLoss: 3.5990796089172363\n",
      "[Epoch 1/100  ][Batch 87 ]\tLoss: 5.440601348876953\n",
      "[Epoch 1/100  ][Batch 88 ]\tLoss: 2.609182119369507\n",
      "[Epoch 1/100  ][Batch 89 ]\tLoss: 4.3496503829956055\n",
      "[Epoch 1/100  ][Batch 90 ]\tLoss: 14.836362838745117\n",
      "[Epoch 1/100  ][Batch 91 ]\tLoss: 4.881239891052246\n",
      "[Epoch 1/100  ][Batch 92 ]\tLoss: 3.7833504676818848\n",
      "[Epoch 1/100  ][Batch 93 ]\tLoss: 2.710937023162842\n",
      "[Epoch 1/100  ][Batch 94 ]\tLoss: 2.8872218132019043\n",
      "[Epoch 1/100  ][Batch 95 ]\tLoss: 3.263885974884033\n",
      "[Epoch 1/100  ][Batch 96 ]\tLoss: 2.4169435501098633\n",
      "[Epoch 1/100  ][Batch 97 ]\tLoss: 9.139513969421387\n",
      "[Epoch 1/100  ][Batch 98 ]\tLoss: 1.194494605064392\n",
      "[Epoch 1/100  ][Batch 99 ]\tLoss: 1.5282171964645386\n",
      "[Epoch 1/100  ][Batch 100]\tLoss: 2.6601874828338623\n",
      "[Epoch 1/100  ][Batch 101]\tLoss: 2.6641693115234375\n",
      "[Epoch 1/100  ][Batch 102]\tLoss: 0.6426759362220764\n",
      "[Epoch 1/100  ][Batch 103]\tLoss: 2.375969886779785\n",
      "[Epoch 1/100  ][Batch 104]\tLoss: 2.8841161727905273\n",
      "[Epoch 1/100  ][Batch 105]\tLoss: 5.38734245300293\n",
      "[Epoch 1/100  ][Batch 106]\tLoss: 2.0472874641418457\n",
      "[Epoch 1/100  ][Batch 107]\tLoss: 1.709386944770813\n",
      "[Epoch 1/100  ][Batch 108]\tLoss: 0.9114700555801392\n",
      "[Epoch 1/100  ][Batch 109]\tLoss: 4.47296142578125\n",
      "[Epoch 1/100  ][Batch 110]\tLoss: 0.7133115530014038\n",
      "[Epoch 1/100  ][Batch 111]\tLoss: 3.459411859512329\n",
      "[Epoch 1/100  ][Batch 112]\tLoss: 3.5160112380981445\n",
      "[Epoch 1/100  ][Batch 113]\tLoss: 0.536287784576416\n",
      "[Epoch 1/100  ][Batch 114]\tLoss: 2.372089147567749\n",
      "[Epoch 1/100  ][Batch 115]\tLoss: 2.818990707397461\n",
      "[Epoch 1/100  ][Batch 116]\tLoss: 0.787360429763794\n",
      "[Epoch 1/100  ][Batch 117]\tLoss: 1.8177039623260498\n",
      "[Epoch 1/100  ][Batch 118]\tLoss: 4.505908966064453\n",
      "[Epoch 1/100  ][Batch 119]\tLoss: 1.2170078754425049\n",
      "[Epoch 1/100  ][Batch 120]\tLoss: 2.3748319149017334\n",
      "[Epoch 1/100  ][Batch 121]\tLoss: 1.287022590637207\n",
      "[Epoch 1/100  ][Batch 122]\tLoss: 0.7665354609489441\n",
      "[Epoch 1/100  ][Batch 123]\tLoss: 0.24426919221878052\n",
      "[Epoch 2/100  ][Batch 0  ]\tLoss: 2.5147833824157715\n",
      "[Epoch 2/100  ][Batch 1  ]\tLoss: 2.9627790451049805\n",
      "[Epoch 2/100  ][Batch 2  ]\tLoss: 1.1037046909332275\n",
      "[Epoch 2/100  ][Batch 3  ]\tLoss: 0.9144353866577148\n",
      "[Epoch 2/100  ][Batch 4  ]\tLoss: 1.9988785982131958\n",
      "[Epoch 2/100  ][Batch 5  ]\tLoss: 1.270416021347046\n",
      "[Epoch 2/100  ][Batch 6  ]\tLoss: 1.9224964380264282\n",
      "[Epoch 2/100  ][Batch 7  ]\tLoss: 1.5427228212356567\n",
      "[Epoch 2/100  ][Batch 8  ]\tLoss: 0.7995758652687073\n",
      "[Epoch 2/100  ][Batch 9  ]\tLoss: 1.2402597665786743\n",
      "[Epoch 2/100  ][Batch 10 ]\tLoss: 0.8241522908210754\n",
      "[Epoch 2/100  ][Batch 11 ]\tLoss: 1.3800911903381348\n",
      "[Epoch 2/100  ][Batch 12 ]\tLoss: 5.331216812133789\n",
      "[Epoch 2/100  ][Batch 13 ]\tLoss: 1.3526517152786255\n",
      "[Epoch 2/100  ][Batch 14 ]\tLoss: 1.0896062850952148\n",
      "[Epoch 2/100  ][Batch 15 ]\tLoss: 1.6180235147476196\n",
      "[Epoch 2/100  ][Batch 16 ]\tLoss: 1.060518741607666\n",
      "[Epoch 2/100  ][Batch 17 ]\tLoss: 2.1622931957244873\n",
      "[Epoch 2/100  ][Batch 18 ]\tLoss: 1.366749882698059\n",
      "[Epoch 2/100  ][Batch 19 ]\tLoss: 1.8388619422912598\n",
      "[Epoch 2/100  ][Batch 20 ]\tLoss: 0.42049849033355713\n",
      "[Epoch 2/100  ][Batch 21 ]\tLoss: 1.735457181930542\n",
      "[Epoch 2/100  ][Batch 22 ]\tLoss: 0.3234044909477234\n",
      "[Epoch 2/100  ][Batch 23 ]\tLoss: 0.9234718680381775\n",
      "[Epoch 2/100  ][Batch 24 ]\tLoss: 0.46552038192749023\n",
      "[Epoch 2/100  ][Batch 25 ]\tLoss: 0.9303339719772339\n",
      "[Epoch 2/100  ][Batch 26 ]\tLoss: 2.366711378097534\n",
      "[Epoch 2/100  ][Batch 27 ]\tLoss: 0.7699207663536072\n",
      "[Epoch 2/100  ][Batch 28 ]\tLoss: 0.8797913789749146\n",
      "[Epoch 2/100  ][Batch 29 ]\tLoss: 0.5515087842941284\n",
      "[Epoch 2/100  ][Batch 30 ]\tLoss: 0.9800236225128174\n",
      "[Epoch 2/100  ][Batch 31 ]\tLoss: 1.126208782196045\n",
      "[Epoch 2/100  ][Batch 32 ]\tLoss: 1.4394525289535522\n",
      "[Epoch 2/100  ][Batch 33 ]\tLoss: 8.419476509094238\n",
      "[Epoch 2/100  ][Batch 34 ]\tLoss: 2.515277147293091\n",
      "[Epoch 2/100  ][Batch 35 ]\tLoss: 2.260563611984253\n",
      "[Epoch 2/100  ][Batch 36 ]\tLoss: 1.4477466344833374\n",
      "[Epoch 2/100  ][Batch 37 ]\tLoss: 2.2246482372283936\n",
      "[Epoch 2/100  ][Batch 38 ]\tLoss: 3.8509609699249268\n",
      "[Epoch 2/100  ][Batch 39 ]\tLoss: 1.1814998388290405\n",
      "[Epoch 2/100  ][Batch 40 ]\tLoss: 2.403061628341675\n",
      "[Epoch 2/100  ][Batch 41 ]\tLoss: 3.4173471927642822\n",
      "[Epoch 2/100  ][Batch 42 ]\tLoss: 1.149130940437317\n",
      "[Epoch 2/100  ][Batch 43 ]\tLoss: 1.1658536195755005\n",
      "[Epoch 2/100  ][Batch 44 ]\tLoss: 1.6719343662261963\n",
      "[Epoch 2/100  ][Batch 45 ]\tLoss: 1.6974213123321533\n",
      "[Epoch 2/100  ][Batch 46 ]\tLoss: 1.2998509407043457\n",
      "[Epoch 2/100  ][Batch 47 ]\tLoss: 0.7838615775108337\n",
      "[Epoch 2/100  ][Batch 48 ]\tLoss: 2.126877784729004\n",
      "[Epoch 2/100  ][Batch 49 ]\tLoss: 0.3891918361186981\n",
      "[Epoch 2/100  ][Batch 50 ]\tLoss: 1.142789363861084\n",
      "[Epoch 2/100  ][Batch 51 ]\tLoss: 0.9821844100952148\n",
      "[Epoch 2/100  ][Batch 52 ]\tLoss: 2.6093342304229736\n",
      "[Epoch 2/100  ][Batch 53 ]\tLoss: 1.6011128425598145\n",
      "[Epoch 2/100  ][Batch 54 ]\tLoss: 1.1460168361663818\n",
      "[Epoch 2/100  ][Batch 55 ]\tLoss: 1.0286234617233276\n",
      "[Epoch 2/100  ][Batch 56 ]\tLoss: 0.7258821129798889\n",
      "[Epoch 2/100  ][Batch 57 ]\tLoss: 0.17813019454479218\n",
      "[Epoch 2/100  ][Batch 58 ]\tLoss: 1.6029095649719238\n",
      "[Epoch 2/100  ][Batch 59 ]\tLoss: 0.3869589567184448\n",
      "[Epoch 2/100  ][Batch 60 ]\tLoss: 0.7514551281929016\n",
      "[Epoch 2/100  ][Batch 61 ]\tLoss: 0.7091573476791382\n",
      "[Epoch 2/100  ][Batch 62 ]\tLoss: 0.3310159146785736\n",
      "[Epoch 2/100  ][Batch 63 ]\tLoss: 0.297721803188324\n",
      "[Epoch 2/100  ][Batch 64 ]\tLoss: 0.635028600692749\n",
      "[Epoch 2/100  ][Batch 65 ]\tLoss: 0.6600956916809082\n",
      "[Epoch 2/100  ][Batch 66 ]\tLoss: 0.43237459659576416\n",
      "[Epoch 2/100  ][Batch 67 ]\tLoss: 1.0305242538452148\n",
      "[Epoch 2/100  ][Batch 68 ]\tLoss: 0.5456665754318237\n",
      "[Epoch 2/100  ][Batch 69 ]\tLoss: 0.4311331510543823\n",
      "[Epoch 2/100  ][Batch 70 ]\tLoss: 0.5913199782371521\n",
      "[Epoch 2/100  ][Batch 71 ]\tLoss: 0.2542566657066345\n",
      "[Epoch 2/100  ][Batch 72 ]\tLoss: 0.5019142627716064\n",
      "[Epoch 2/100  ][Batch 73 ]\tLoss: 0.2722479999065399\n",
      "[Epoch 2/100  ][Batch 74 ]\tLoss: 0.3842141032218933\n",
      "[Epoch 2/100  ][Batch 75 ]\tLoss: 0.41994720697402954\n",
      "[Epoch 2/100  ][Batch 76 ]\tLoss: 0.9117689728736877\n",
      "[Epoch 2/100  ][Batch 77 ]\tLoss: 0.36393123865127563\n",
      "[Epoch 2/100  ][Batch 78 ]\tLoss: 1.829268455505371\n",
      "[Epoch 2/100  ][Batch 79 ]\tLoss: 0.5054972767829895\n",
      "[Epoch 2/100  ][Batch 80 ]\tLoss: 0.48163774609565735\n",
      "[Epoch 2/100  ][Batch 81 ]\tLoss: 0.5317409634590149\n",
      "[Epoch 2/100  ][Batch 82 ]\tLoss: 0.4878782033920288\n",
      "[Epoch 2/100  ][Batch 83 ]\tLoss: 0.6045699119567871\n",
      "[Epoch 2/100  ][Batch 84 ]\tLoss: 0.44356322288513184\n",
      "[Epoch 2/100  ][Batch 85 ]\tLoss: 0.2708515524864197\n",
      "[Epoch 2/100  ][Batch 86 ]\tLoss: 0.32146766781806946\n",
      "[Epoch 2/100  ][Batch 87 ]\tLoss: 0.41087156534194946\n",
      "[Epoch 2/100  ][Batch 88 ]\tLoss: 0.350615531206131\n",
      "[Epoch 2/100  ][Batch 89 ]\tLoss: 0.5215728282928467\n",
      "[Epoch 2/100  ][Batch 90 ]\tLoss: 0.6117093563079834\n",
      "[Epoch 2/100  ][Batch 91 ]\tLoss: 0.2414787858724594\n",
      "[Epoch 2/100  ][Batch 92 ]\tLoss: 0.4335295259952545\n",
      "[Epoch 2/100  ][Batch 93 ]\tLoss: 0.4340333640575409\n",
      "[Epoch 2/100  ][Batch 94 ]\tLoss: 0.5476531982421875\n",
      "[Epoch 2/100  ][Batch 95 ]\tLoss: 0.45004311203956604\n",
      "[Epoch 2/100  ][Batch 96 ]\tLoss: 0.28602632880210876\n",
      "[Epoch 2/100  ][Batch 97 ]\tLoss: 0.24796544015407562\n",
      "[Epoch 2/100  ][Batch 98 ]\tLoss: 0.2876828610897064\n",
      "[Epoch 2/100  ][Batch 99 ]\tLoss: 1.326540470123291\n",
      "[Epoch 2/100  ][Batch 100]\tLoss: 0.8407099843025208\n",
      "[Epoch 2/100  ][Batch 101]\tLoss: 0.6068879961967468\n",
      "[Epoch 2/100  ][Batch 102]\tLoss: 0.46947118639945984\n",
      "[Epoch 2/100  ][Batch 103]\tLoss: 0.36228832602500916\n",
      "[Epoch 2/100  ][Batch 104]\tLoss: 0.5613746643066406\n",
      "[Epoch 2/100  ][Batch 105]\tLoss: 0.5430170893669128\n",
      "[Epoch 2/100  ][Batch 106]\tLoss: 0.4381125867366791\n",
      "[Epoch 2/100  ][Batch 107]\tLoss: 1.9279935359954834\n",
      "[Epoch 2/100  ][Batch 108]\tLoss: 0.7040010094642639\n",
      "[Epoch 2/100  ][Batch 109]\tLoss: 0.28968966007232666\n",
      "[Epoch 2/100  ][Batch 110]\tLoss: 0.4587380290031433\n",
      "[Epoch 2/100  ][Batch 111]\tLoss: 0.31953704357147217\n",
      "[Epoch 2/100  ][Batch 112]\tLoss: 0.3769005537033081\n",
      "[Epoch 2/100  ][Batch 113]\tLoss: 0.42289435863494873\n",
      "[Epoch 2/100  ][Batch 114]\tLoss: 0.5303691625595093\n",
      "[Epoch 2/100  ][Batch 115]\tLoss: 0.5014054775238037\n",
      "[Epoch 2/100  ][Batch 116]\tLoss: 1.3582830429077148\n",
      "[Epoch 2/100  ][Batch 117]\tLoss: 0.5349524617195129\n",
      "[Epoch 2/100  ][Batch 118]\tLoss: 0.48923614621162415\n",
      "[Epoch 2/100  ][Batch 119]\tLoss: 0.39685922861099243\n",
      "[Epoch 2/100  ][Batch 120]\tLoss: 0.38476961851119995\n",
      "[Epoch 2/100  ][Batch 121]\tLoss: 0.5198490619659424\n",
      "[Epoch 2/100  ][Batch 122]\tLoss: 0.18991361558437347\n",
      "[Epoch 2/100  ][Batch 123]\tLoss: 0.34532344341278076\n",
      "[Epoch 3/100  ][Batch 0  ]\tLoss: 0.37976333498954773\n",
      "[Epoch 3/100  ][Batch 1  ]\tLoss: 0.4316883981227875\n",
      "[Epoch 3/100  ][Batch 2  ]\tLoss: 0.4518929421901703\n",
      "[Epoch 3/100  ][Batch 3  ]\tLoss: 0.5308696031570435\n",
      "[Epoch 3/100  ][Batch 4  ]\tLoss: 0.3065904378890991\n",
      "[Epoch 3/100  ][Batch 5  ]\tLoss: 0.5710848569869995\n",
      "[Epoch 3/100  ][Batch 6  ]\tLoss: 0.6658947467803955\n",
      "[Epoch 3/100  ][Batch 7  ]\tLoss: 0.31341353058815\n",
      "[Epoch 3/100  ][Batch 8  ]\tLoss: 0.3107702136039734\n",
      "[Epoch 3/100  ][Batch 9  ]\tLoss: 0.3202967345714569\n",
      "[Epoch 3/100  ][Batch 10 ]\tLoss: 0.7629327774047852\n",
      "[Epoch 3/100  ][Batch 11 ]\tLoss: 0.7308012843132019\n",
      "[Epoch 3/100  ][Batch 12 ]\tLoss: 0.37355467677116394\n",
      "[Epoch 3/100  ][Batch 13 ]\tLoss: 0.42171168327331543\n",
      "[Epoch 3/100  ][Batch 14 ]\tLoss: 0.6975154876708984\n",
      "[Epoch 3/100  ][Batch 15 ]\tLoss: 0.3051436245441437\n",
      "[Epoch 3/100  ][Batch 16 ]\tLoss: 0.27659985423088074\n",
      "[Epoch 3/100  ][Batch 17 ]\tLoss: 0.38586851954460144\n",
      "[Epoch 3/100  ][Batch 18 ]\tLoss: 0.4411293864250183\n",
      "[Epoch 3/100  ][Batch 19 ]\tLoss: 0.3960162103176117\n",
      "[Epoch 3/100  ][Batch 20 ]\tLoss: 0.21819932758808136\n",
      "[Epoch 3/100  ][Batch 21 ]\tLoss: 0.558756947517395\n",
      "[Epoch 3/100  ][Batch 22 ]\tLoss: 0.3730945885181427\n",
      "[Epoch 3/100  ][Batch 23 ]\tLoss: 0.3328165113925934\n",
      "[Epoch 3/100  ][Batch 24 ]\tLoss: 0.16605626046657562\n",
      "[Epoch 3/100  ][Batch 25 ]\tLoss: 0.2458498328924179\n",
      "[Epoch 3/100  ][Batch 26 ]\tLoss: 0.18017849326133728\n",
      "[Epoch 3/100  ][Batch 27 ]\tLoss: 0.29292988777160645\n",
      "[Epoch 3/100  ][Batch 28 ]\tLoss: 0.15228018164634705\n",
      "[Epoch 3/100  ][Batch 29 ]\tLoss: 0.34935441613197327\n",
      "[Epoch 3/100  ][Batch 30 ]\tLoss: 0.33014169335365295\n",
      "[Epoch 3/100  ][Batch 31 ]\tLoss: 0.5234920382499695\n",
      "[Epoch 3/100  ][Batch 32 ]\tLoss: 0.42001646757125854\n",
      "[Epoch 3/100  ][Batch 33 ]\tLoss: 0.19830825924873352\n",
      "[Epoch 3/100  ][Batch 34 ]\tLoss: 0.3122970163822174\n",
      "[Epoch 3/100  ][Batch 35 ]\tLoss: 0.37116941809654236\n",
      "[Epoch 3/100  ][Batch 36 ]\tLoss: 0.4531036913394928\n",
      "[Epoch 3/100  ][Batch 37 ]\tLoss: 0.8729705214500427\n",
      "[Epoch 3/100  ][Batch 38 ]\tLoss: 0.3437879979610443\n",
      "[Epoch 3/100  ][Batch 39 ]\tLoss: 0.31312811374664307\n",
      "[Epoch 3/100  ][Batch 40 ]\tLoss: 0.3599947988986969\n",
      "[Epoch 3/100  ][Batch 41 ]\tLoss: 0.22197651863098145\n",
      "[Epoch 3/100  ][Batch 42 ]\tLoss: 0.44668149948120117\n",
      "[Epoch 3/100  ][Batch 43 ]\tLoss: 0.48526138067245483\n",
      "[Epoch 3/100  ][Batch 44 ]\tLoss: 0.27810215950012207\n",
      "[Epoch 3/100  ][Batch 45 ]\tLoss: 0.37393367290496826\n",
      "[Epoch 3/100  ][Batch 46 ]\tLoss: 0.31642425060272217\n",
      "[Epoch 3/100  ][Batch 47 ]\tLoss: 0.3025982975959778\n",
      "[Epoch 3/100  ][Batch 48 ]\tLoss: 0.30960381031036377\n",
      "[Epoch 3/100  ][Batch 49 ]\tLoss: 0.3967881500720978\n",
      "[Epoch 3/100  ][Batch 50 ]\tLoss: 0.2603702247142792\n",
      "[Epoch 3/100  ][Batch 51 ]\tLoss: 0.24091844260692596\n",
      "[Epoch 3/100  ][Batch 52 ]\tLoss: 0.265794038772583\n",
      "[Epoch 3/100  ][Batch 53 ]\tLoss: 0.3882177174091339\n",
      "[Epoch 3/100  ][Batch 54 ]\tLoss: 0.619540274143219\n",
      "[Epoch 3/100  ][Batch 55 ]\tLoss: 0.3125758767127991\n",
      "[Epoch 3/100  ][Batch 56 ]\tLoss: 0.3921498954296112\n",
      "[Epoch 3/100  ][Batch 57 ]\tLoss: 0.20318616926670074\n",
      "[Epoch 3/100  ][Batch 58 ]\tLoss: 0.3164600729942322\n",
      "[Epoch 3/100  ][Batch 59 ]\tLoss: 0.35593345761299133\n",
      "[Epoch 3/100  ][Batch 60 ]\tLoss: 0.3520321846008301\n",
      "[Epoch 3/100  ][Batch 61 ]\tLoss: 0.273002564907074\n",
      "[Epoch 3/100  ][Batch 62 ]\tLoss: 0.2819420397281647\n",
      "[Epoch 3/100  ][Batch 63 ]\tLoss: 0.17080537974834442\n",
      "[Epoch 3/100  ][Batch 64 ]\tLoss: 0.3831644654273987\n",
      "[Epoch 3/100  ][Batch 65 ]\tLoss: 0.3095838725566864\n",
      "[Epoch 3/100  ][Batch 66 ]\tLoss: 0.10540034621953964\n",
      "[Epoch 3/100  ][Batch 67 ]\tLoss: 0.5275313258171082\n",
      "[Epoch 3/100  ][Batch 68 ]\tLoss: 0.3660196363925934\n",
      "[Epoch 3/100  ][Batch 69 ]\tLoss: 0.4154016077518463\n",
      "[Epoch 3/100  ][Batch 70 ]\tLoss: 0.5007435083389282\n",
      "[Epoch 3/100  ][Batch 71 ]\tLoss: 0.34318894147872925\n",
      "[Epoch 3/100  ][Batch 72 ]\tLoss: 0.30834195017814636\n",
      "[Epoch 3/100  ][Batch 73 ]\tLoss: 0.2230873852968216\n",
      "[Epoch 3/100  ][Batch 74 ]\tLoss: 0.2974749207496643\n",
      "[Epoch 3/100  ][Batch 75 ]\tLoss: 0.5129044055938721\n",
      "[Epoch 3/100  ][Batch 76 ]\tLoss: 0.23722299933433533\n",
      "[Epoch 3/100  ][Batch 77 ]\tLoss: 0.317164808511734\n",
      "[Epoch 3/100  ][Batch 78 ]\tLoss: 0.34485888481140137\n",
      "[Epoch 3/100  ][Batch 79 ]\tLoss: 0.29478925466537476\n",
      "[Epoch 3/100  ][Batch 80 ]\tLoss: 0.3160207271575928\n",
      "[Epoch 3/100  ][Batch 81 ]\tLoss: 0.27336224913597107\n",
      "[Epoch 3/100  ][Batch 82 ]\tLoss: 0.29136204719543457\n",
      "[Epoch 3/100  ][Batch 83 ]\tLoss: 0.2572782635688782\n",
      "[Epoch 3/100  ][Batch 84 ]\tLoss: 0.24828705191612244\n",
      "[Epoch 3/100  ][Batch 85 ]\tLoss: 0.2575652301311493\n",
      "[Epoch 3/100  ][Batch 86 ]\tLoss: 0.2955780625343323\n",
      "[Epoch 3/100  ][Batch 87 ]\tLoss: 0.4027584195137024\n",
      "[Epoch 3/100  ][Batch 88 ]\tLoss: 0.34764590859413147\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 59.12 MiB is free. Process 15890 has 53.91 MiB memory in use. Process 28457 has 1.22 GiB memory in use. Including non-PyTorch memory, this process has 18.74 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 873.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \ty_train[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2) Forward Propagation\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3) loss calculation\u001b[39;00m\n\u001b[1;32m     20\u001b[0m curr_loss \u001b[38;5;241m=\u001b[39m loss(y_pred, y_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/coding-projects/kaggle-spine-competition/model_1.py:51\u001b[0m, in \u001b[0;36mAvgPoolingCnn.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m series:\n\u001b[1;32m     50\u001b[0m     t \u001b[38;5;241m=\u001b[39m conv_layer(t)\n\u001b[0;32m---> 51\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(t, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     53\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(t, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 59.12 MiB is free. Process 15890 has 53.91 MiB memory in use. Process 28457 has 1.22 GiB memory in use. Including non-PyTorch memory, this process has 18.74 GiB memory in use. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 873.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 1) Set up loss function and optimizer\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train() # Set model to train mode (turn on gradient tracking)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "\tfor batch_idx, (X_train, y_train) in enumerate(dl):\n",
    "\t\tfor k, v in X_train.items():\n",
    "\t\t\tX_train[k] = v.to(DEVICE)\n",
    "\t\tfor k, v in y_train.items():\n",
    "\t\t\ty_train[k] = v.to(DEVICE)\n",
    "\n",
    "\t\t# 2) Forward Propagation\n",
    "\t\ty_pred = model(X_train)\n",
    "\t\t# break\n",
    "\n",
    "\t\t# 3) loss calculation\n",
    "\t\tcurr_loss = loss(y_pred, y_train[\"SCS\"])\n",
    "\n",
    "\t\t# 4) Clear out optimizer gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# 5) Back Propagation\n",
    "\t\tcurr_loss.backward()\n",
    "\n",
    "\t\t# 6) Optimizer step\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tprint(f\"[Epoch {f'{epoch+1}/{epochs}':<7}][Batch {batch_idx:<3}]\\tLoss: {curr_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(model, 'models/SCS-model-dict.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
